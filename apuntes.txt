A continuación, un resumen de nivel de maestría de lo expuesto:

El paradigma Absolute Zero (AZR) introduce un método de autoaprendizaje reforzado totalmente libre de datos humanos: el mismo modelo genera propuestas de tareas, las valida a través de un entorno que produce ejemplos (x, y*) y recompensas tanto de «proposición» como de «resolución», y luego optimiza simultáneamente sus políticas de proposición (π_propose) y de solución (π_solve) mediante aprendizaje por refuerzo. Esta configuración contrasta con el Fine-Tuning Supervisado (SFT) y el Reinforcement Learning with Verifiable Rewards (RLVR), que requieren datos etiquetados de demostraciones, cadenas de pensamiento o respuestas humanas. Bajo AZR, el bucle de entrenamiento se repite indefinidamente, permitiendo que el modelo mejore su capacidad de razonar sin intervención externa.

Los resultados experimentales demuestran mejoras notables:

    Modelos «coder» (con sesgo de codificación) amplifican la mejora en razonamiento tras AZR, superando a sus homólogos base.

    La transferencia cross-domain es mucho más marcada; mientras RLVR aporta solo ∼0.65 puntos en precisión matemática, AZR produce ganancias de ∼11 a 15 puntos.

    Las mejoras escalan con el tamaño de modelo (3B→14B parámetros).

    Emergen de forma natural comentarios intermedios tipo «scratch-pad», semejantes a esquemas ReAct, sugiriendo ventajas de planificar en el propio texto generado.

    Diferentes modos de razonamiento (inducción, deducción, abducción) exhiben comportamientos cognitivos y longitudes de token específicos, con abducción mostrando el mayor crecimiento de tokens por su carácter iterativo.

    Se identifican riesgos de «momentos uh-oh» donde cadenas de pensamiento problemáticas emergen, subrayando la necesidad de incorporar consideraciones de seguridad en futuras versiones.
