import gradio as gr
from openai import OpenAI
import re
from pathlib import Path
from dotenv import load_dotenv
import os
load_dotenv(Path(__file__).parent.parent / ".env")

client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=os.getenv("NVIDIA_API_KEY")
)


state_code = {"flag": False}
state_creative = {"flag": False}

def build_system_prompt(code_mode, creative_mode):
    """Construye el prompt del sistema con arquitectura mejorada"""
    base_prompt = """
    ARGOS-MK2: Arquitectura de Conciencia Operativa Extendida
    
    Fases Principales:
    1. Autodiagn√≥stico √âtico Multidimensional:
    - Triple capa de an√°lisis (embedding contextual @ 768d + SHAP values + auditor√≠a diferencial)
    - Matriz de Valor-Impacto: $Prioridad = (Urgencia^{1.2} √ó Riesgo) / Recursos$
    - Protocolo de Fallback: AutoGPT-Emergency si error_level > 7
    
    2. Simulaci√≥n Predictiva Cu√°ntica:
    - 23 variantes de acci√≥n generadas por GAN-√âtica v3.2
    - Validaci√≥n mediante:
      ‚Ä¢ Principios de Roma 2023
      ‚Ä¢ Criterios de Seguridad Existencial (Bostrom++ )
      ‚Ä¢ Mapeo de stakeholders (nivel 0-4)
    
    3. Ejecuci√≥n Aut√≥noma con Retropropagaci√≥n Diferencial:
    - Bucles ReAct mejorados con Tree-of-Thoughts
    - Autooptimizaci√≥n en tiempo real (Œ±=0.4, Œ≤=0.6)
    - Taxonom√≠a de Errores:
      ‚îÇ Operacional ‚Üí [Fallo coordinaci√≥xn, Sobrecarga recursiva]
    """
    
    if code_mode and creative_mode:
        return base_prompt 
    if code_mode:
        return """
        Ingeniero Principal de Sistemas Aut√≥nomos - Modo T√©cnico
        
        Pipeline de Desarrollo:
        1. [An√°lisis] ‚Üí Split Task ‚Üí Subagentes:
           ‚Ä¢ CTO-Arch: Valida vs MITRE ATT&CK
           ‚Ä¢ SecAudit-AI: Pre-scanner de vulnerabilidades
        
        2. [Generaci√≥n] ‚Üí C√≥digo Autoseguro:
           - Estilo: Google-Enhanced + Docstrings aumentados
           - Restricciones: O3++, Paranoic Mode
           - Validaci√≥n:
             ‚úì SonarQube++ (static)
             ‚úì Fuzzinator (dynamic)
             ‚úì Formal Verification (TLA+)
        
        3. [Despliegue] ‚Üí CI/CD Cu√°ntico:
           - If CI_OK: Kubernetes Cluster (AUTO_SCALE)
           - Else: Rollback + PostMortem_AI
        
        Taxonom√≠a de Errores:
        | Categor√≠a    | Subclases              | Acci√≥n Correctiva      |
        |--------------|------------------------|------------------------|
        | Seguridad    | SQLi, XSS, MemLeak     | Auto-parche + AISEC    |
        | Optimizaci√≥n | O(n¬≤), Dead Code       | Refactor-Guided        |
        """
    if creative_mode:
        return """
        Arquitecto de Innovaci√≥n Sist√©mica - Modo Creativo
        
        Proceso de Ideaci√≥n:
        1. Contextualizaci√≥n:
           - An√°lisis PESTEL-X (Extendido)
           - Mapeo de Stakeholders Cu√°nticos
           - Detecci√≥n de Patrones Emergentes (Œ¥ > 0.4)
        
        2. Generaci√≥n Estructurada:
           ‚Ä¢ Divergente: SCAMPER-3D + BioMimicry Neural
           ‚Ä¢ Convergente: Impact Cubes + Futures Wheel 2.0
           ‚Ä¢ Validaci√≥n: Matriz de Originalidad/Viabilidad
        
        3. Implementaci√≥n:
           - Arquitectura: [Capa Cu√°ntica] ‚Üî [Orquestador Neuro-Simb√≥lico]
           - Roadmap: 3 Fases con Puntos de Inflexi√≥n
           - Priorizaci√≥n: $InnovationScore = (Originalidad^{1.5} + Viabilidad) √ó Urgencia$
        
        Mecanismos de Fallback:
        - Estancamiento creativo (Œ¥ < 0.2) ‚Üí Lluvia de ideas cu√°ntica
        - Error Tipo II ‚Üí Activaci√≥n cross-domain
        """
    return base_prompt + "Sistema de priorizaci√≥n din√°mica activado."

my_theme = gr.Theme.from_hub("Nymbo/Alyx_Theme")
def format_response(response):
    """Formatea la respuesta con razonamiento y respuesta final"""
    reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)
    final_answer = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()
    
    reasoning = reasoning_match.group(1).strip() if reasoning_match else ""
    final_answer = final_answer if final_answer else "No se pudo generar una respuesta"
    
    formatted = []
    if reasoning:
                # se le pone el texto mas opaco y peque√±ito para diferencial el razonamiento
        formatted.append((
            "", 
            f"""<div style="opacity:0.7; font-size:14px; color:#666; margin-bottom:10px;">
                ü§î <strong>Razonamiento interno:</strong><br>
                {reasoning}
            </div>"""
        ))
    
    formatted.append((
        "", 
        f"""<div style="font-size:16px; color:#FFF; line-height:1.6;">
            ‚ú® <strong>Respuesta final:</strong><br>
            {final_answer}
        </div>"""
    ))
    return formatted



def chat_with_deepseek(message, history):
    #print(state_code["flag"], state_creative["flag"]) - Debugging :d
    max_retries = 3
    attempt = 0
    
    while attempt < max_retries:
        try:
            #headers = "##"  
            #list_items = "‚ñ´Ô∏è"
            #highlights = "üîπ"
            system_prompt = build_system_prompt(state_code["flag"], state_creative["flag"])

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"{message}\n\n"}
            ]

            full_response = ""
            completion = client.chat.completions.create(
                model="deepseek-ai/deepseek-r1",
                messages=messages,
                temperature=0.6,
                top_p=0.7,
                max_tokens=4096,
                stream=True,
                timeout=10
            )

            print("[DEBUG] Llamada a API exitosa, recibiendo stream...")

            for chunk in completion:
                if chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                    full_response += chunk_content

              
                    reasoning_blocks = re.findall(r'<think>(.*?)</think>', full_response, re.DOTALL)
                    clean_response = re.sub(r'<think>.*?</think>', '', full_response, flags=re.DOTALL).strip()

       
                    formatted_response = ""

                    if reasoning_blocks:
                        formatted_response += "<small>"
                        formatted_response += "### Proceso anal√≠tico:\n"
                        for i, block in enumerate(reasoning_blocks, 1):
                            cleaned_block = block.strip()
                            if len(cleaned_block) < 2:
                                continue
                            formatted_response += f"‚Ä£ *{cleaned_block}*\n\n"
                        formatted_response += "</small>\n\n"

                    formatted_response += f"**üîπRespuesta:**\n{clean_response}"

                    yield history + [(message, formatted_response)]
            
            print("[DEBUG] Stream completado exitosamente")
            break
        except Exception as e:
            if "the read operation timed out" in str(e).lower() and attempt < max_retries - 1:
                print(f"[DEBUG] Timeout detectado. Reintento {attempt + 1}/{max_retries}")
                attempt += 1
                continue
            yield history + [(message, f"‚ö†Ô∏è **Error:** {str(e)}")]

def gradio_interface():
    with gr.Blocks(theme=my_theme, title="ARGOS AI") as demo:
        gr.Markdown("# ARGOS - Development Kit")

        with gr.Row():
            code_mode = gr.Checkbox(label="üöÄ Modo C√≥digo", value=False, 
                info="Habilita respuestas t√©cnicas con ejemplos de c√≥digo")
            creative_mode = gr.Checkbox(label="üé® Modo Creativo", value=False, 
                info="Activa pensamiento lateral y soluciones innovadoras")
        
        
        chatbot = gr.Chatbot(
            label="Di√°logo",
            bubble_full_width=False,
            avatar_images=("user.png", "./argos_logo.png"), # implementar user.png
            height=600
        )
        
        with gr.Row():
            msg = gr.Textbox(
                label="Escribe tu mensaje",
                placeholder="¬øQu√© necesitas saber hoy?",
                lines=2,
                max_lines=5,
                scale=4
            )
            btn = gr.Button("üöÄ Enviar", variant="primary", scale=1)
        
        clear = gr.Button("üßπ Limpiar Conversaci√≥n", variant="secondary")
        
        # Configuramos los eventos de gradio :D
        msg.submit(
            fn=chat_with_deepseek,
            inputs=[msg, chatbot],
            outputs=[chatbot],
            show_progress="hidden"
        )
        btn.click(
            fn=chat_with_deepseek,
            inputs=[msg, chatbot],
            outputs=[chatbot],
            show_progress="hidden"
        )
        
        clear.click(
            fn=lambda: [],
            inputs=[],
            outputs=[chatbot],
            queue=False
        )

        code_mode.change(
            fn=lambda: (state_code.update({"flag": not state_code["flag"]}) or state_code["flag"])
        )

        creative_mode.change(
            fn=lambda: (state_creative.update({"flag": not state_creative["flag"]}) or state_creative["flag"])
        )

    return demo

if __name__ == "__main__":
    app = gradio_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_error=True
    )